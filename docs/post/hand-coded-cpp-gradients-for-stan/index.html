<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Scott Spencer">
<meta name="description" content="I wrote a tutorial on a technique that isn&rsquo;t in the Stan documentation: hand-coding C&#43;&#43; gradients to bypass Stan&rsquo;s automatic differentiation system.
The short version: Stan&rsquo;s autodiff is general-purpose and excellent. But when a model has millions of repetitive operations &mdash; gathering indexed parameters, computing the same likelihood across 1.5 million observations &mdash; the bookkeeping overhead can dominate the actual math. You can replace that overhead with analytic gradients, using the same internal mechanism Stan&rsquo;s own developers use for built-in functions.
" />
<meta name="keywords" content="Scott Spencer, Bayesian modeling, Stan, sports analytics, Columbia University, MLB, Premier League, physics-informed models, generative modeling, Stan, C&#43;&#43;, autodiff, optimization, Bayesian modeling" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://ssp3nc3r.github.io/post/hand-coded-cpp-gradients-for-stan/" />


<script async src="https://www.googletagmanager.com/gtag/js?id=G-1V94CLF78W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1V94CLF78W');
</script>


    <title>
        
            Hand-coded C&#43;&#43; gradients for Stan :: Dr. Scott Spencer 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://ssp3nc3r.github.io/main.min.d814cf833213d446ba235aa22dd9d7b7ea71e9f96be092ffac602d0030f4f2c2.css">


    
        <link rel="stylesheet" type="text/css" href="https://ssp3nc3r.github.io/css/custom.css">
    



    <link rel="apple-touch-icon" sizes="180x180" href="https://ssp3nc3r.github.io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://ssp3nc3r.github.io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://ssp3nc3r.github.io/favicon-16x16.png">
    <link rel="manifest" href="https://ssp3nc3r.github.io/site.webmanifest">
    <link rel="mask-icon" href="https://ssp3nc3r.github.io/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://ssp3nc3r.github.io/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="Hand-coded C&#43;&#43; gradients for Stan">
  <meta itemprop="description" content="I wrote a tutorial on a technique that isn’t in the Stan documentation: hand-coding C&#43;&#43; gradients to bypass Stan’s automatic differentiation system.
The short version: Stan’s autodiff is general-purpose and excellent. But when a model has millions of repetitive operations — gathering indexed parameters, computing the same likelihood across 1.5 million observations — the bookkeeping overhead can dominate the actual math. You can replace that overhead with analytic gradients, using the same internal mechanism Stan’s own developers use for built-in functions.">
  <meta itemprop="datePublished" content="2026-02-15T00:00:00+00:00">
  <meta itemprop="dateModified" content="2026-02-15T00:00:00+00:00">
  <meta itemprop="wordCount" content="179">
  <meta itemprop="image" content="https://ssp3nc3r.github.io/">
  <meta itemprop="keywords" content="Stan,C&#43;&#43;,Autodiff,Optimization,Bayesian Modeling">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://ssp3nc3r.github.io/">
  <meta name="twitter:title" content="Hand-coded C&#43;&#43; gradients for Stan">
  <meta name="twitter:description" content="I wrote a tutorial on a technique that isn’t in the Stan documentation: hand-coding C&#43;&#43; gradients to bypass Stan’s automatic differentiation system.
The short version: Stan’s autodiff is general-purpose and excellent. But when a model has millions of repetitive operations — gathering indexed parameters, computing the same likelihood across 1.5 million observations — the bookkeeping overhead can dominate the actual math. You can replace that overhead with analytic gradients, using the same internal mechanism Stan’s own developers use for built-in functions.">



    <meta property="article:section" content="Stan" />

    <meta property="article:section" content="Bayesian" />

    <meta property="article:section" content="C&#43;&#43;" />

    <meta property="article:section" content="Performance" />



    <meta property="article:published_time" content="2026-02-15 00:00:00 &#43;0000 UTC" />








        <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>
    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://ssp3nc3r.github.io/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text"> update_beliefs()</span>
            <span class="logo__cursor" style=
                  "
                   background-color:#67a2c9;
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://ssp3nc3r.github.io/about/">About</a></li><li><a href="https://ssp3nc3r.github.io/post/">Blog</a></li><li><a href="https://ssp3nc3r.github.io/publications/">Publications</a></li><li><a href="https://ssp3nc3r.github.io/teaching/">Teaching</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
    
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://ssp3nc3r.github.io/post/hand-coded-cpp-gradients-for-stan/">Hand-coded C++ gradients for Stan</a></h2>

            

            <div class="post-content">
                <p>I wrote a <a href="https://ssp3nc3r.github.io/publications/papers/stan-custom-cpp-tutorial.html">tutorial</a> on a technique that isn&rsquo;t in the Stan documentation: hand-coding C++ gradients to bypass Stan&rsquo;s automatic differentiation system.</p>
<p>The short version: Stan&rsquo;s autodiff is general-purpose and excellent. But when a model has millions of repetitive operations &mdash; gathering indexed parameters, computing the same likelihood across 1.5 million observations &mdash; the bookkeeping overhead can dominate the actual math. You can replace that overhead with analytic gradients, using the same internal mechanism Stan&rsquo;s own developers use for built-in functions.</p>
<p>The tutorial walks through the full pattern, from a toy weighted sum to indexed gather-scatter operations to parallelism with <code>reduce_sum</code> and raw TBB. It includes a real-world case study from a hierarchical model of global football across 85+ leagues and a decade of seasons, where this technique cut the dominant likelihoods by 10x and turned a multi-day run into something more practical for iterative development.</p>
<p><img src="https://ssp3nc3r.github.io/post/hand-coded-cpp-gradients-for-stan/images/stan-custom-cpp-likelihood-speedup-comparison.jpeg" alt="Performance comparison of pure Stan vs. custom C++ gradients across model likelihoods, showing up to 10.6x speedup."></p>
<p>It&rsquo;s the kind of optimization that only matters after you&rsquo;ve already done everything else &mdash; vectorized, parallelized, profiled. But when you&rsquo;re there, it&rsquo;s the difference between waiting for results and working with them.</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
  				<p>
  					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://ssp3nc3r.github.io/tags/stan">Stan</a></span><span class="tag"><a href="https://ssp3nc3r.github.io/tags/c&#43;&#43;">C&#43;&#43;</a></span><span class="tag"><a href="https://ssp3nc3r.github.io/tags/autodiff">autodiff</a></span><span class="tag"><a href="https://ssp3nc3r.github.io/tags/optimization">optimization</a></span><span class="tag"><a href="https://ssp3nc3r.github.io/tags/bayesian-modeling">Bayesian modeling</a></span>
  				</p>
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            
            <span></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
        </div>
    </div>
    <script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</footer>

            
        </div>

        




<script type="text/javascript" src="https://ssp3nc3r.github.io/bundle.min.74fe699bb673e8362137b575513abfcf73303855d923eea09c0e507deab0ca7f8321880b672790b9e63cc109a18189deebfd13899a8ff536e858791973ffd487.js" integrity="sha512-dP5pm7Zz6DYhN7V1UTq/z3MwOFXZI&#43;6gnA5Qfeqwyn&#43;DIYgLZyeQueY8wQmhgYne6/0TiZqP9TboWHkZc//Uhw=="></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'G-1V94CLF78W', 'auto');
        ga('send', 'pageview');
    </script>


	
		<script src="js/typewriter.js"></script>
	


    </body>
</html>
